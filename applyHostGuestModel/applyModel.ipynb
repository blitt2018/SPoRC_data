{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, RobertaForTokenClassification\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model class \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        #def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        #since we have three classes \n",
    "        self.l1 = nn.Linear(768, 3)\n",
    "\n",
    "        #normalizes probabilities to sum to 1\n",
    "        self.sig = nn.Sigmoid()\n",
    "        #self.ixList = ixList\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    #outIndices tells us the indices of the tokens corresponding to our word of interest\n",
    "    #for each instance in our batch\n",
    "    def forward(self, input_ids, attention_mask, outIndices): \n",
    "        \n",
    "        #encode sentence and get mean pooled sentence representation \n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        #now we just have outIndices come to us in the forward pass \n",
    "        #outIndices = [self.ixList[ix] for ix in index]\n",
    "        embeddingMeans = []\n",
    "        batchIter = 0\n",
    "        for batchIter in range(input_ids.shape[0]): \n",
    "            \n",
    "            #get the last layer of the model \n",
    "            hiddenStates = output[0]\n",
    "            \n",
    "            #get the embeddings corresponding to the entity we're interested in \n",
    "            tokStates = [hiddenStates[batchIter][tokIndex,:] for tokIndex in outIndices[batchIter]]\n",
    "            \n",
    "            #take the mean over all embeddings for an entity \n",
    "            embeddingMean = torch.stack(tokStates).mean(dim=0)\n",
    "            \n",
    "            #append this so we get the mean embedding for each \n",
    "            #training example in this batch \n",
    "            embeddingMeans.append(embeddingMean) \n",
    "            #embeddingMeans.append(hiddenStates[batchIter][outIndices[batchIter][0],:])\n",
    "        \n",
    "        #we stack because this is for an entire batch \n",
    "        embeddingMeans = torch.stack(embeddingMeans)\n",
    "        \"\"\"\n",
    "        working code just used this!\n",
    "        embeddingMeans = self.mean_pooling(output[0], attention_mask)\n",
    "        \"\"\"\n",
    "        probs = self.sig(self.l1(embeddingMeans)).squeeze()\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load best model from training\n",
    "STATE_PATH  = \"/shared/3/projects/benlitterer/podcastData/hostGuestModels/initialModel/bestF1Params\"\n",
    "deviceNum = 1\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(torch.load(STATE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_json(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthDataClean.jsonl\", orient=\"records\", lines=True, nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toKeep = [\"potentialOutPath\", \"transcript\", \"rssUrl\", \"epTitle\", \"epDescription\", \"transEnts\", \"transStarts\", \"transEnds\", \"transTypes\"]\n",
    "df = df[toKeep].explode([\"transEnts\", \"transStarts\", \"transEnds\", \"transTypes\"])\n",
    "df = df[df[\"transTypes\"] == \"PERSON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing mentions after a certain point \n",
    "#first get the number of words before the entity, we only use < 350 to train, so go with that \n",
    "def getEntPos(inRow): \n",
    "    return len(inRow[\"transcript\"][:inRow[\"transStarts\"]].split())\n",
    "\n",
    "df[\"entPos\"] = df.apply(getEntPos, axis=1)\n",
    "df = df[df[\"entPos\"] < 350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transEntLen\"] = df[\"transEnts\"].apply(lambda x: len(x.split()))\n",
    "df = df[df[\"transEntLen\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9854, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"potentialOutPath\", \"transEnts\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'transcript'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transcript'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m         buffEnd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(afterWords) \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [buffStart, snippet[entStart:entEnd], buffEnd]\n\u001b[0;32m---> 28\u001b[0m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetSnippet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(), index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mgetSnippet\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetSnippet\u001b[39m(row): \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#find where the entity starts quick \u001b[39;00m\n\u001b[1;32m      6\u001b[0m    \u001b[38;5;66;03m# row[\"snippetStart\"] = trainDf.apply(lambda x: x[\"entSnippets\"].lower().find(x[\"ent\"].lower()), axis=1)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     snippet \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m     entStart \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransStarts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     entEnd \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransEnds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transcript'"
     ]
    }
   ],
   "source": [
    "\n",
    "BEFORE_BUFF = 50\n",
    "AFTER_BUFF=50\n",
    "#PUNCH IN HERE\n",
    "def getSnippet(row): \n",
    "    #find where the entity starts quick \n",
    "   # row[\"snippetStart\"] = trainDf.apply(lambda x: x[\"entSnippets\"].lower().find(x[\"ent\"].lower()), axis=1)\n",
    "    \n",
    "    snippet = row[\"transcript\"]\n",
    "    entStart = row[\"transStarts\"]\n",
    "    entEnd = row[\"transEnds\"]\n",
    "\n",
    "    \n",
    "    beforeWords = snippet[0:entStart].split(\" \")\n",
    "    if len(beforeWords) >= BEFORE_BUFF: \n",
    "        buffStart = \" \".join(beforeWords[-BEFORE_BUFF:]) \n",
    "    else: \n",
    "        buffStart = \" \".join(beforeWords) \n",
    "\n",
    "    afterWords = snippet[entEnd:len(snippet)].split(\" \")\n",
    "\n",
    "    if len(afterWords) >= AFTER_BUFF: \n",
    "        buffEnd = \" \".join(afterWords[:AFTER_BUFF]) \n",
    "    else: \n",
    "        buffEnd = \" \".join(afterWords) \n",
    "    return [buffStart, snippet[entStart:entEnd], buffEnd]\n",
    "            \n",
    "\n",
    "df[[\"left\", \"ent\", \"right\"]] = pd.DataFrame(df.apply(getSnippet, axis=1).tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the sake of memory \n",
    "df = df.drop(columns=[\"transcript\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entSnippets\"] = df[\"left\"] + df[\"ent\"] + df[\"right\"] \n",
    "\n",
    "#df = df[[\"left\", \"right\", \"ent\",'transStarts', 'transEnds', 'groundTruth', 'entSnippets']]\n",
    "#df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"snippetStart\"] = df.apply(lambda x: x[\"entSnippets\"].lower().find(x[\"ent\"].lower()), axis=1)\n",
    "df[\"snippetEnd\"] = df[\"snippetStart\"] + df[\"transEnds\"] - df[\"transStarts\"]\n",
    "\n",
    "def extractEnt(inRow): \n",
    "    return inRow[\"entSnippets\"][inRow[\"snippetStart\"]:inRow[\"snippetEnd\"]]\n",
    "\n",
    "df[\"extractedEnt\"] = df.apply(extractEnt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length=512, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for snip in df[\"entSnippets\"]: \n",
    "    tokenized.append(tokenizer(snip, padding = \"max_length\", truncation=True, return_offsets_mapping=True))\n",
    "\n",
    "df = pd.concat([df.reset_index(), pd.DataFrame.from_records(tokenized)], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the token indices which correspond to our entity \n",
    "def getTokenIndices(start, end, offsets):\n",
    "    \"\"\"\n",
    "    print(start) \n",
    "    print(end) \n",
    "    print(offsets[:20]) \n",
    "    \"\"\"\n",
    "\n",
    "    currIndices = []\n",
    "    for j, offset in enumerate(offsets): \n",
    "        offsetL, offsetR = offset\n",
    "        if offsetL >= start and offsetR <= end: \n",
    "            currIndices.append(j)\n",
    "\n",
    "    return currIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"posTokens\"] = df.apply(lambda row: getTokenIndices(row[\"snippetStart\"], row[\"snippetEnd\"], row[\"offset_mapping\"]), axis=1)\n",
    "\n",
    "#drop extra information about location of tokens that aren't those of interest\n",
    "df = df.drop(columns=[\"offset_mapping\"])\n",
    "\n",
    "labList = []\n",
    "for i, row in df.iterrows(): \n",
    "    tokCount = sum(row[\"attention_mask\"])\n",
    "    paddingLen = len(row[\"attention_mask\"]) - tokCount\n",
    "    \n",
    "    labels = ([0] * tokCount) + ([2] * paddingLen)\n",
    "    \n",
    "    for posIndex in row[\"posTokens\"]: \n",
    "        labels[posIndex] = 1\n",
    "    \n",
    "    labList.append(labels) \n",
    "\n",
    "df[\"labels\"] = labList\n",
    "\n",
    "df[\"entsTokenized\"] = df.apply(lambda row: [tokenizer.decode(row[\"input_ids\"][i]) for i in row[\"posTokens\"]], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities where we have error from extraction of entity: 0\n"
     ]
    }
   ],
   "source": [
    "extractionErrorCount = len(df[df[\"extractedEnt\"].apply(lambda x: x.lower()) != df[\"ent\"].apply(lambda x: x.lower())]) \n",
    "print(f\"Number of entities where we have error from extraction of entity: {extractionErrorCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"attention_mask\", \"input_ids\", \"posTokens\"])\n",
    "\n",
    "#add an index to be used for getting the position of tokens later \n",
    "df = df.reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenRef = dict(zip(df[\"index\"], df[\"posTokens\"]))\n",
    "tokenIxList = list(df[\"posTokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"index\"])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2464 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2464/2464 [04:34<00:00,  8.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#input ids, mask, indices \n",
    "predList = []\n",
    "probList = []\n",
    "#TODO: remove full transcript column here to save memory \n",
    "for batch in tqdm(loader):\n",
    "    input_ids = batch[\"input_ids\"].to(device) \n",
    "    attention_mask = batch[\"attention_mask\"].to(device) \n",
    "    index = batch[\"index\"]\n",
    "    outIndices = [tokenIxList[i] for i in index]\n",
    "    \n",
    "    probs = model(input_ids, attention_mask, outIndices) #.to(torch.float32)\n",
    "\n",
    "    #if the last batch has only 1 row, we need to add another dimension in \n",
    "    if len(probs.shape) == 1: \n",
    "        probs = probs.unsqueeze(0)\n",
    "    \n",
    "    probList += probs.to(\"cpu\").tolist()\n",
    "    preds = torch.max(probs, 1).indices.to(int).cpu().tolist()\n",
    "    predList += preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred\"] = predList\n",
    "df[\"prob\"] = probList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = \"/shared/3/projects/benlitterer/podcastData/hostIdentification/hostGuestPredictions/10000LongPredictions.json\"\n",
    "df[[\"potentialOutPath\", \"rssUrl\", \"ent\", \"pred\", \"prob\"]].to_json(outPath, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMode(inList): \n",
    "    if len(inList) == 1: \n",
    "        return inList[0]\n",
    "    \n",
    "    data = Counter(inList)\n",
    "    modeVal, modeCount = data.most_common(1)[0]\n",
    "\n",
    "    #we default to neither if we have a split decision\n",
    "    \n",
    "    if modeCount == 1: \n",
    "        return 2\n",
    "    else: \n",
    "        return modeVal \n",
    "    \n",
    "    return modeVal\n",
    "\n",
    "#here we take the index of the maximum probability prediction \n",
    "#after mean pooling over columns \n",
    "def getConfidenceAggregation(inList): \n",
    "    inList = np.array(inList)\n",
    "    return np.argmax(np.mean(inList, axis=0))\n",
    "\n",
    "#we take in a 2d array of shape n x 3\n",
    "#get the prediction for the row with the highest probability \n",
    "def getMostConfident(inList): \n",
    "\n",
    "    maxVal = 0 \n",
    "    maxValIx = 2\n",
    "    for row in inList: \n",
    "        for colNum, item in enumerate(row): \n",
    "\n",
    "            #if we have a new highest value, update \n",
    "            #note that maxValIx is just our prediction of 0, 1, or 2\n",
    "            if item > maxVal: \n",
    "                maxVal = item \n",
    "                maxValIx = colNum\n",
    "    return maxValIx\n",
    "\n",
    "aggDf = df[[\"potentialOutPath\", \"ent\", \"pred\", \"prob\"]].groupby([\"potentialOutPath\", \"ent\"]).agg(list)\n",
    "aggDf[\"modalPred\"] = aggDf[\"pred\"].apply(getMode)\n",
    "aggDf[\"confPred\"] = aggDf[\"prob\"].apply(getMostConfident)\n",
    "aggDf[\"meanAggPred\"] = aggDf[\"prob\"].apply(getConfidenceAggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggArr = aggDf[[\"modalPred\", \"confPred\", \"meanAggPred\"]].T.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.96811818, 0.9708293 ],\n",
       "       [0.96811818, 1.        , 0.99684888],\n",
       "       [0.9708293 , 0.99684888, 1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(aggArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potentialOutPath</th>\n",
       "      <th>ent</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/anchor.fm/0a/httpsanchor.fms59db584podcastplay13749364https3A2F2Fd3ctxlq1ktw2nl.cloudfront.net2Fstaging2F202005142F995a5a1ac4131000030f0993e5afc00f.m4aMERGED</th>\n",
       "      <th>Don Chanel</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">/anchor.fm/20/httpsanchor.fms126c0978podcastplay14603313https3A2F2Fd3ctxlq1ktw2nl.cloudfront.net2Fproduction2F20204312F7840600244100257f2a88169a22.m4aMERGED</th>\n",
       "      <th>Brody Myers</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micah Wilcox</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/anchor.fm/20/httpsanchor.fms2e2929cpodcastplay14311720https3A2F2Fd3ctxlq1ktw2nl.cloudfront.net2Fproduction2F20204262F76791003480002c47e0a8a50627.mp3MERGED</th>\n",
       "      <th>Shadow Shkowski</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/anchor.fm/20/httpsanchor.fms67be020podcastplay14204913https3A2F2Fd3ctxlq1ktw2nl.cloudfront.net2Fproduction2F20204242F7612718532000181158a6abd0e8.mp3MERGED</th>\n",
       "      <th>Eppie Ludwig</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/traffic.megaphone.fm/6A/httpstraffic.megaphone.fmAPO6964160656.mp3MERGED</th>\n",
       "      <th>Tony Gill</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/traffic.megaphone.fm/73/httpstraffic.megaphone.fmAPO7738284721.mp3MERGED</th>\n",
       "      <th>Anita Johnston</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/traffic.megaphone.fm/86/httpstraffic.megaphone.fmAPO8028686917.mp3MERGED</th>\n",
       "      <th>Brooke Castillo's</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/www.podtrac.com/re/httpswww.podtrac.comptsredirect.mp3pdst.fmer.zencastr.comrtraffic.megaphone.fmHUMAN2742163750.mp3updated1666811397MERGED</th>\n",
       "      <th>Chris Gray</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/www.podtrac.com/re/httpswww.podtrac.comptsredirect.mp3pdst.fmer.zencastr.comrtraffic.megaphone.fmHUMAN4509755542.mp3updated1666811409MERGED</th>\n",
       "      <th>Rebecca Murphy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      pred\n",
       "potentialOutPath                                   ent                    \n",
       "/anchor.fm/0a/httpsanchor.fms59db584podcastplay... Don Chanel            0\n",
       "/anchor.fm/20/httpsanchor.fms126c0978podcastpla... Brody Myers           0\n",
       "                                                   Micah Wilcox          1\n",
       "/anchor.fm/20/httpsanchor.fms2e2929cpodcastplay... Shadow Shkowski       1\n",
       "/anchor.fm/20/httpsanchor.fms67be020podcastplay... Eppie Ludwig          1\n",
       "...                                                                    ...\n",
       "/traffic.megaphone.fm/6A/httpstraffic.megaphone... Tony Gill             1\n",
       "/traffic.megaphone.fm/73/httpstraffic.megaphone... Anita Johnston        1\n",
       "/traffic.megaphone.fm/86/httpstraffic.megaphone... Brooke Castillo's     1\n",
       "/www.podtrac.com/re/httpswww.podtrac.comptsredi... Chris Gray            1\n",
       "/www.podtrac.com/re/httpswww.podtrac.comptsredi... Rebecca Murphy        1\n",
       "\n",
       "[164 rows x 1 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPath = \"/shared/3/projects/benlitterer/podcastData/hostIdentification/hostGuestPredictions/1000predictions.json\"\n",
    "aggDf.to_json(outPath, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
